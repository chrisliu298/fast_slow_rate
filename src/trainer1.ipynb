{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trainer1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4lg4hLENfiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Models\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "from tensorflow.keras.layers import GlobalMaxPool1D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Dataset\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Others\n",
        "from statistics import mean\n",
        "import json\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "def plot(history, string):\n",
        "    \"\"\"\n",
        "    Plot training acc/loss and validation acc/loss\n",
        "    \"\"\"\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history[\"val_\" + string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([\"train_\" + string, \"valid_\" + string])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class GridSearch:\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_size,\n",
        "        valid_size,\n",
        "        test_size,\n",
        "        lstm_search_space,\n",
        "        dense_search_space,\n",
        "        dropout_search_space,\n",
        "        batch_size,\n",
        "        learning_rate,\n",
        "    ):\n",
        "        # Parameters\n",
        "        self.train_size = train_size\n",
        "        self.valid_size = valid_size\n",
        "        self.test_size = test_size\n",
        "        self.lstm_search_space = lstm_search_space\n",
        "        self.dense_search_space = dense_search_space\n",
        "        self.dropout_search_space = dropout_search_space\n",
        "        self.vocab_size = 20000\n",
        "        self.embedding_dim = 50\n",
        "        self.max_len = 512\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = 50\n",
        "        self.patience = 10\n",
        "        self.seed = 42\n",
        "        self.trunc_type = \"post\"\n",
        "        self.padding = \"pre\"\n",
        "        self.oov_token = \"<OOV>\"\n",
        "        self.callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor=\"val_accuracy\",\n",
        "                patience=self.patience,\n",
        "                restore_best_weights=True,\n",
        "            )\n",
        "        ]\n",
        "        # Load the dataset\n",
        "        imdb = tfds.load(\"imdb_reviews\", as_supervised=True, shuffle_files=True)\n",
        "        self.train_dataset = imdb[\"train\"]\n",
        "        self.test_dataset = imdb[\"test\"]\n",
        "\n",
        "    def prepare_data(self):\n",
        "        train_text = []\n",
        "        train_labels = []\n",
        "        test_text = []\n",
        "        test_labels = []\n",
        "\n",
        "        # Put sentences and labels in lists\n",
        "        for s, l in self.train_dataset:\n",
        "            train_text.append(str(s.numpy()))\n",
        "            train_labels.append(l.numpy())\n",
        "        for s, l in self.test_dataset:\n",
        "            test_text.append(str(s.numpy()))\n",
        "            test_labels.append(l.numpy())\n",
        "\n",
        "        # Convert them into numpy arrays\n",
        "        train_text, train_labels, test_text, test_labels = (\n",
        "            np.array(train_text),\n",
        "            np.array(train_labels),\n",
        "            np.array(test_text),\n",
        "            np.array(test_labels),\n",
        "        )\n",
        "\n",
        "        # Shuffle the train/test set\n",
        "        train_rand = np.arange(len(train_text))\n",
        "        np.random.shuffle(train_rand)\n",
        "        train_text = train_text[train_rand]\n",
        "        train_labels = train_labels[train_rand]\n",
        "        test_rand = np.arange(len(test_text))\n",
        "        np.random.shuffle(test_rand)\n",
        "        test_text = test_text[test_rand]\n",
        "        test_labels = test_labels[test_rand]\n",
        "\n",
        "        # Take the subset of the data\n",
        "        train_reviews, valid_reviews = (\n",
        "            train_text[: self.train_size],\n",
        "            train_text[-self.valid_size :],\n",
        "        )\n",
        "        train_sentiments, valid_sentiments = (\n",
        "            np.array(train_labels[: self.train_size]),\n",
        "            np.array(train_labels[-self.valid_size :]),\n",
        "        )\n",
        "        test_reviews = test_text[: self.test_size]\n",
        "        test_sentiments = np.array(test_labels[: self.test_size])\n",
        "\n",
        "        tokenizer = Tokenizer(num_words=self.vocab_size, oov_token=self.oov_token)\n",
        "        tokenizer.fit_on_texts(train_reviews)\n",
        "\n",
        "        train_seq = tokenizer.texts_to_sequences(train_reviews)\n",
        "        train_padded = pad_sequences(\n",
        "            train_seq,\n",
        "            maxlen=self.max_len,\n",
        "            truncating=self.trunc_type,\n",
        "            padding=self.padding,\n",
        "        )\n",
        "        valid_seq = tokenizer.texts_to_sequences(valid_reviews)\n",
        "        valid_padded = pad_sequences(\n",
        "            valid_seq,\n",
        "            maxlen=self.max_len,\n",
        "            truncating=self.trunc_type,\n",
        "            padding=self.padding,\n",
        "        )\n",
        "        test_seq = tokenizer.texts_to_sequences(test_reviews)\n",
        "        test_padded = pad_sequences(\n",
        "            test_seq,\n",
        "            maxlen=self.max_len,\n",
        "            truncating=self.trunc_type,\n",
        "            padding=self.padding,\n",
        "        )\n",
        "\n",
        "        print(\"Training sentences count:\", len(train_padded))\n",
        "        print(\"Training labels count:\", len(train_sentiments))\n",
        "        print(\"Validation sentences count:\", len(valid_padded))\n",
        "        print(\"Validation labels count:\", len(valid_sentiments))\n",
        "        print(\"Test sentences count:\", len(test_padded))\n",
        "        print(\"Testing labels count:\", len(test_sentiments))\n",
        "        return [\n",
        "            (train_padded, train_sentiments),\n",
        "            (valid_padded, valid_sentiments),\n",
        "            (test_padded, test_sentiments),\n",
        "        ]\n",
        "\n",
        "    def build_model(self, lstm_hidden_size, dense_hidden_size, dropout_rate):\n",
        "        model = Sequential()\n",
        "        model.add(\n",
        "            Embedding(self.vocab_size, self.embedding_dim, input_length=self.max_len)\n",
        "        )\n",
        "        model.add(Bidirectional(LSTM(lstm_hidden_size, return_sequences=True)))\n",
        "        model.add(GlobalMaxPool1D())\n",
        "        model.add(Dense(dense_hidden_size, activation=\"relu\"))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "        model.add(Dense(1, activation=\"sigmoid\"))\n",
        "        model.compile(\n",
        "            loss=\"binary_crossentropy\",\n",
        "            optimizer=Adam(learning_rate=1e-3),\n",
        "            metrics=[\"accuracy\"],\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def train(self, data):\n",
        "        (\n",
        "            (train_padded, train_sentiments),\n",
        "            (valid_padded, valid_sentiments),\n",
        "            (test_padded, test_sentiments),\n",
        "        ) = data\n",
        "        histories = []\n",
        "        counter = 0\n",
        "        for lstm_size in self.lstm_search_space:\n",
        "            for dense_size in self.dense_search_space:\n",
        "                for dropout_rate in self.dropout_search_space:\n",
        "                    model = self.build_model(lstm_size, dense_size, dropout_rate)\n",
        "                    counter += 1\n",
        "                    print(f\"========== Trial {counter} Summary ==========\")\n",
        "                    print(f\"   lstm_search_space = {lstm_size}\")\n",
        "                    print(f\"  dense_search_space = {dense_size}\")\n",
        "                    print(f\"dropout_search_space = {dropout_rate}\\n\")\n",
        "                    history = model.fit(\n",
        "                        train_padded,\n",
        "                        train_sentiments,\n",
        "                        validation_data=(valid_padded, valid_sentiments),\n",
        "                        batch_size=self.batch_size,\n",
        "                        epochs=self.epochs,\n",
        "                        shuffle=True,\n",
        "                        callbacks=[self.callbacks],\n",
        "                        verbose=0\n",
        "                    )\n",
        "                    log = {\n",
        "                        \"lstm_hidden_size\": lstm_size,\n",
        "                        \"dense_hidden_size\": dense_size,\n",
        "                        \"dropout_rate\": dropout_rate,\n",
        "                        \"train_acc_converge\": history.history[\"accuracy\"][-11],\n",
        "                        \"train_acc_final\": history.history[\"accuracy\"][-1],\n",
        "                        \"valid_acc_converge\": history.history[\"val_accuracy\"][-11],\n",
        "                        \"valid_acc_final\": history.history[\"val_accuracy\"][-1],\n",
        "                        \"max_valid_acc\": max(history.history[\"val_accuracy\"]),\n",
        "                    }\n",
        "                    histories.append(log)\n",
        "                    print()\n",
        "                    print(f\"========== Trial {counter} Results ==========\")\n",
        "                    print(json.dumps(log, indent=4))\n",
        "                    print()\n",
        "        return histories\n",
        "\n",
        "    def get_best_model(self, history):\n",
        "        return max(history, key=lambda x: x[\"max_valid_acc\"])\n",
        "\n",
        "    def train_best_model(self, best_params, num_trials=100):\n",
        "        best_lstm_size = best_params[\"lstm_hidden_size\"]\n",
        "        best_dense_size = best_params[\"dense_hidden_size\"]\n",
        "        best_dropout_rate = best_params[\"dropout_rate\"]\n",
        "        (train_padded, train_sentiments), (valid_padded, valid_sentiments), (test_padded, test_sentiments) = self.prepare_data()\n",
        "        train_acc = []\n",
        "        final_train_acc = []\n",
        "        valid_acc = []\n",
        "        final_valid_acc = []\n",
        "        test_acc = []\n",
        "        for t in range(num_trials):\n",
        "            print(f\"========== Tiral {t + 1} ==========\")\n",
        "            model = self.build_model(best_lstm_size, best_dense_size, best_dropout_rate)\n",
        "            history = model.fit(\n",
        "                train_padded,\n",
        "                train_sentiments,\n",
        "                validation_data=(valid_padded, valid_sentiments),\n",
        "                batch_size=self.batch_size,\n",
        "                epochs=self.epochs,\n",
        "                shuffle=True,\n",
        "                callbacks=self.callbacks,\n",
        "                verbose=0\n",
        "            )\n",
        "            loss, acc = model.evaluate(test_padded, test_sentiments, batch_size=self.batch_size)\n",
        "            train_acc.append(history.history[\"accuracy\"][-11])\n",
        "            final_train_acc.append(history.history[\"accuracy\"][-1])\n",
        "            valid_acc.append(history.history[\"val_accuracy\"][-11])\n",
        "            final_valid_acc.append(history.history[\"val_accuracy\"][-1])\n",
        "            test_acc.append(acc)\n",
        "            print()\n",
        "        return (train_acc, final_train_acc, valid_acc, final_valid_acc, test_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VsAwqFeEep2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = timer()\n",
        "search = GridSearch(\n",
        "    train_size=100,\n",
        "    valid_size=2000,\n",
        "    test_size=12500,\n",
        "    lstm_search_space=[16, 32, 64, 128, 256],\n",
        "    dense_search_space=[16, 32, 64, 128, 256],\n",
        "    dropout_search_space=[0.0, 0.1, 0.2, 0.3, 0.4],\n",
        "    batch_size=32,\n",
        "    learning_rate=3e-4,\n",
        ")\n",
        "\n",
        "data = search.prepare_data()\n",
        "history = search.train(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvqa0vbcPicb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_params = search.get_best_model(history)\n",
        "print(json.dumps(best_params, indent=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApYezz9Yur6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_acc, final_train_acc, valid_acc, final_valid_acc, test_acc) = search.train_best_model(best_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_BDD3Xv1mhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statistics import mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIk4SQtw1nqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(search.train_size)\n",
        "print(round(mean(train_acc), 4))\n",
        "print(round(mean(final_train_acc), 4))\n",
        "print(round(mean(valid_acc), 4))\n",
        "print(round(mean(final_valid_acc), 4))\n",
        "print(round(mean(test_acc), 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONuLHZoO9p4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "end = timer()\n",
        "print((end - start) / 3600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbt77x7jlkqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}